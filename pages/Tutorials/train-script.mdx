# Train Script and Code Migration

You are now ready to start writing code to begin training your model. Start off by writing all training related scripts in
the `train_script.py` file.

Training using `Bench AI` is the exact same as training in regular pytorch.

```python filename="train_script.py" copy
import torch
from benchkit.data.helpers import get_dataloader # Turns a IterableChunk to a pytorch Dataloader
from datasets.project_datasets import MyChunkDS # The IterableChunk dataset
from models.project_models import MyModel
import torch.nn as nn
import torch.optim as opt
from torch.utils.data import DataLoader


def train_one_epoch(train_dl: DataLoader,
                    model: nn.Module,
                    optim: torch.optim.Optimizer,
                    loss_fn: torch.nn.BCELoss,
                    device: torch.device) -> torch.Tensor:

    """
    Trains a model for one entire epoch, and returns the average loss
    """

    model.train() # Ensure that weight modification and dynamic behaviors is on

    loss_tensor = torch.zeros(size=(len(train_dl),),
                              device=device,
                              dtype=torch.float32) # used to hold the loss of each batch

    for idx, batch in enumerate(train_dl):

        optim.zero_grad() # resets the gradients to zero before back propagation
        targets, inputs = batch

        """
        Makes sure inputs and a targets are of the right datatype, and are on the right device
        """

        inputs: torch.Tensor = inputs.to(device)
        targets: torch.Tensor = torch.unsqueeze(targets, dim=1)
        targets = targets.type(torch.FloatTensor)
        targets = targets.to(device)

        outputs = model(inputs)

        loss = loss_fn(outputs, targets)
        loss.backward(loss) # Back propagates the loss
        optim.step() # Optimizer takes a gradient step

        loss_tensor[idx] = loss

    return torch.mean(loss_tensor) # the average loss of the epoch


def validate_one_epoch(test_dl: DataLoader,
                       model: nn.Module,
                       loss_fn: torch.nn.BCELoss,
                       device: torch.device):

    """
    Tests the model, and returns the average loss
    """

    model.eval() # Offs weight modification and dynamic behaviours

    loss_tensor = torch.zeros(size=(len(test_dl),),
                              device=device,
                              dtype=torch.float32) # Stores the loss of every batch

    with torch.no_grad(): # Ensures gradient modifications do not take place. Extra precaution

        for idx, batch in enumerate(test_dl):
            targets, inputs = batch

            """
            Makes sure inputs and a targets are of the right datatype, and are on the right device
            """

            inputs: torch.Tensor = inputs.to(device)
            targets: torch.Tensor = torch.unsqueeze(targets, dim=1)
            targets = targets.type(torch.FloatTensor)
            targets = targets.to(device)

            outputs = model(inputs)
            loss: torch.Tensor = loss_fn(outputs, targets)

            loss_tensor[idx] = loss

    return torch.mean(loss_tensor) # returns the average loss

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Ensures Cuda is used

    loss_fn = nn.BCELoss().to(device) # Binary Cross Entropy Loss. Useful for categorical data
    model = MyModel(64).to(device)

    optim = opt.Adam(params=model.parameters(),
                     lr=1e-2)

    tracker_config = {
        "epochs": 10,
        "bs": 16,
        "lr": 1e-2,
        "loss": str(loss_fn),
    } # A baseline configuration of hyperparameters used to train this model

    """
    Turn IterableChunk's to Dataloaders
    """
    train_chunker = MyChunkDS()
    train_dataset: DataLoader = get_dataloader(train_chunker,
                                               "HH_Train_V1")

    val_chunker = MyChunkDS()
    val_dataset: DataLoader = get_dataloader(val_chunker,
                                             "HH_Val_V1")

    for e in range(tracker_config["epochs"]):
        train_one_epoch(train_dataset, model, optim, loss_fn, device)
        validate_one_epoch(val_dataset, model, loss_fn, device)
```

## Config

Bench Ai's use cases don't stop at training your model. We offer a whole suite of tools to help track and understand how your models
are doing. To take advantage of these features start by initializing a model config.

A model config takes in 3 values: the hyperparameters used to train your model, what metric this model should be judged against, and
whether we should maximize or minimize that metric.

```python filename="train_script.py" copy
from benchkit.tracking.config import Config

hyperparameters = {
        "epochs": 10,
        "bs": 16,
        "lr": 1e-2,
        "loss": "BCE_LOSS"} # The hyperparameters you wish to keep track of

model_config = Config(tracker_config, # hyperparams we are using for this model
                      "val_loss", # We will be evaluating this model based on validation loss
                      "min") # We will be trying to minimize validation loss
```

### Saving & Checkpointing

If you wish to save and checkpoint your model use the `save_model_and_state` function of your config. This method will save
a checkpoint everytime this method is called, it will also save the model everytime the evaluation criteria is improved. To
use this model pass in: a function that checkpoints your model to a directory, a function that saves your model to a directory,
the iteration you are on, and the value of the evaluation criteria at that iteration.

Here is an example of how to save checkpoints and saves in vanilla pytorch.

```python filename="train_script.py" copy
from benchkit.tracking.config import Config
from models.project_models import MyModel
from functools import partial
import torch.optim as opt
import torch
import os

def save_model(model: torch.nn.Module,
               example_input: torch.Tensor) -> str:

    """
    Uses Torch's Just in Time compiler to trace and save your model. This allows it to be
    run without needing the model class.
    """

    dir_path = os.path.join(".", "model-artifacts", "save")
    os.makedirs(dir_path, exist_ok=True)

    model_path = os.path.join(dir_path, f"model.pt")

    traced_model = torch.jit.trace(model, example_input) # traces the model, requires a sample input
    traced_model.save(model_path) # saves the model to the directory we specified

    return dir_path # returns the directory containing the traced model

def save_state(epoch: int,
               model: torch.nn.Module,
               optimizer: torch.optim,
               loss: torch.Tensor):

    """
    Saves the state dict and key values of the model for continued training
    """

    path = os.path.join(".", "model-artifacts", "checkpoints", f"{epoch}")
    os.makedirs(path, exist_ok=True)

    tar_path = os.path.join(path, f"checkpoint.tar")

    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, tar_path) # saves all relevant states to the tar_path

    return path # returns the directory containing the tar

def main():
    hyperparameters = {
            "epochs": 10,
            "bs": 16,
            "lr": 1e-2,
            "loss": "BCE_LOSS"} # The hyperparameters you wish to keep track of

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Ensures Cuda is used

    model = MyModel(64).to(device)

    optim = opt.Adam(params=model.parameters(),
                     lr=1e-2)

    model_config = Config(hyperparameters, # hyperparams we are using for this model
                          "val_loss", # We will be evaluating this model based on validation loss
                          "min") # We will be trying to minimize validation loss

    save_model_func = partial(save_model,
                              model,
                              torch.ones(1, 3, 256, 256).to(device)) # makes save_model_func a callable
                                                                     # with all the arguments locked in

    e: int = 0
    val_loss: int = 0

    save_state_func = partial(save_state,
                              e,
                              model,
                              optim,
                              val_loss) # makes save_state a callable with all the arguments locked in

    model_config.save_model_and_state(save_state_func,
                                      save_model_func,
                                      e,
                                      float(val_loss)) # Saves a checkpoint, and a Save if appropriate

```

You can subsequently see all the checkpoints and saves of your model by running

### Graphing
If you wish to visualize how your model is performing you can incorporate one of our trackers. Here we will show how to
use our `TimeSeries` graph.

```python filename="train_script.py" copy
from benchkit.tracking.config import Config
from benchkit.tracking.graphs.time_series import TimeSeries

def main():
    hyperparameters = {
            "epochs": 10,
            "bs": 16,
            "lr": 1e-2,
            "loss": "BCE_LOSS"} # The hyperparameters you wish to keep track of

    model_config = Config(hyperparameters, # hyperparams we are using for this model
                          "val_loss", # We will be evaluating this model based on validation loss
                          "min") # We will be trying to minimize validation loss

    val_loss_graph = TimeSeries("val_loss", # Name of the graph
                                model_config, # The Config for the model being trained
                                "loss", # name of the line
                                "epochs", # x axis name
                                "loss") # y axis name

```

If you wish to have multiple lines in your graph you can do this
```python filename="train_script.py" copy
val_loss_graph = TimeSeries("val_loss", # Name of the graph
                             model_config, # The Config for the model being trained
                             ("l1", "l2"), # name of the lines
                             "epochs", # x axis name
                             "loss") # y axis name
```

To plot a point call `.log_value`
```python filename="train_script.py" copy
"""
Pass in two values, the first will be a dictionary where the key is the name of the line, and the value is the
y-axis value, then pass in a step that is the x-axis value
"""
val_loss_graph.log_value({"loss": 0.0}, step=0)
```

### Full Code

```python filename="train_script.py" copy
from functools import partial

import torch
import os
from benchkit.data.helpers import get_dataloader
from datasets.project_datasets import MyChunkDS
from models.project_models import MyModel
from benchkit.tracking.config import Config
from benchkit.tracking.graphs.time_series import TimeSeries
import torch.nn as nn
import torch.optim as opt
from tqdm import tqdm
from torch.utils.data import DataLoader


def save_model(model: torch.nn.Module,
               example_input: torch.Tensor) -> str:

    dir_path = os.path.join(".", "model-artifacts", "save")
    os.makedirs(dir_path, exist_ok=True)

    model_path = os.path.join(dir_path, f"model.pt")

    traced_model = torch.jit.trace(model, example_input)
    traced_model.save(model_path)

    return dir_path


def save_state(epoch: int,
               model: torch.nn.Module,
               optimizer: torch.optim,
               loss: torch.Tensor):
    path = os.path.join(".", "model-artifacts", "checkpoints", f"{epoch}")
    os.makedirs(path, exist_ok=True)

    tar_path = os.path.join(path, f"checkpoint.tar")

    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, tar_path)

    return path


def train_one_epoch(train_dl: DataLoader,
                    model: nn.Module,
                    optim: torch.optim.Optimizer,
                    loss_fn: torch.nn.BCELoss,
                    device: torch.device) -> torch.Tensor:

    model.train()
    loss_tensor = torch.zeros(size=(len(train_dl),),
                              device=device,
                              dtype=torch.float32)

    for idx, batch in tqdm(enumerate(train_dl),
                              colour="blue"):
        optim.zero_grad()
        targets, inputs = batch

        inputs: torch.Tensor = inputs.to(device)
        targets: torch.Tensor = torch.unsqueeze(targets, dim=1)
        targets = targets.type(torch.FloatTensor)
        targets = targets.to(device)

        outputs = model(inputs)

        loss = loss_fn(outputs, targets)
        loss.backward(loss)
        optim.step()

        loss_tensor[idx] = loss

    return torch.mean(loss_tensor)


def validate_one_epoch(test_dl: DataLoader,
                       model: nn.Module,
                       loss_fn: torch.nn.BCELoss,
                       device: torch.device):
    model.eval()
    loss_tensor = torch.zeros(size=(len(test_dl),),
                              device=device,
                              dtype=torch.float32)

    with torch.no_grad():
        for idx, batch in tqdm(enumerate(test_dl), colour="blue"):
            targets, inputs = batch
            inputs: torch.Tensor = inputs.to(device)
            targets: torch.Tensor = torch.unsqueeze(targets, dim=1)
            targets = targets.type(torch.FloatTensor)
            targets = targets.to(device)

            outputs = model(inputs)
            loss: torch.Tensor = loss_fn(outputs, targets)

            loss_tensor[idx] = loss

    return torch.mean(loss_tensor)


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"The current device is {device}")

    loss_fn = nn.BCELoss().to(device)
    model = MyModel(64).to(device)

    optim = opt.Adam(params=model.parameters(),
                     lr=1e-2)

    tracker_config = {
        "epochs": 10,
        "bs": 16,
        "lr": 1e-2,
        "loss": str(loss_fn),
    }

    model_config = Config(tracker_config,
                          "val_loss",
                          "min")

    train_loss_graph = TimeSeries("train_loss",
                                  model_config,
                                  "loss",
                                  "epochs",
                                  "loss")

    val_loss_graph = TimeSeries("val_loss",
                                model_config,
                                "loss",
                                "epochs",
                                "loss")

    train_chunker = MyChunkDS(True)
    train_dataset: DataLoader = get_dataloader(train_chunker,
                                               "HH_Train_V1")

    val_chunker = MyChunkDS(False)
    val_dataset: DataLoader = get_dataloader(val_chunker,
                                             "HH_Val_V1")

    for e in range(tracker_config["epochs"]):
        train_loss = train_one_epoch(train_dataset, model, optim, loss_fn, device)
        val_loss = validate_one_epoch(val_dataset, model, loss_fn, device)

        train_loss_graph.log_value({"loss": float(train_loss)},
                                   step=e)

        val_loss_graph.log_value({"loss": float(val_loss)},
                                 step=e)

        save_model_func = partial(save_model,
                                  model,
                                  torch.ones(1, 3, 256, 256).to(device))

        save_state_func = partial(save_state,
                                  e,
                                  model,
                                  optim,
                                  val_loss)

        model_config.save_model_and_state(save_state_func,
                                          save_model_func,
                                          e,
                                          float(val_loss))

```


## Migrate Code
Once you are ready to train your model, you can begin the code migration process. Migrating code saves your code to the cloud, so it can be
loaded on any server you want to train with.

Run:
```bash filename="> Terminal" copy
python manage.py migrate-code
```

<video controls loop muted autoPlay>
  <source src="https://sofadocsbucket.s3.us-west-2.amazonaws.com/assets/Tutorials/train-script/migrate-code.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

This command migrates your code with a default version of `1`. This means that if you were to make changes to this code
and were to migrate it, it would overwrite the previous version `1` code you had. All files and folders that were created
with the `start-project` command will be migrated, files not in this category will be left as is.


### Versioning

Versioning becomes extremely useful, when you make changes to your models architecture, data, or training script.
Through versioning you can run multiple experiments of different model versions and can compare the accuracy of each
side by side.

To migrate code with a specific version use this command instead:
```bash filename="> Terminal" copy
python manage.py migrate-code <version: int>
```

To see all your current versions run this command
```bash filename="> Terminal" copy
benchkit show-vs
```
<video controls loop muted autoPlay>
  <source src="https://sofadocsbucket.s3.us-west-2.amazonaws.com/assets/Tutorials/train-script/show-vs.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

You can also see all the versions you have for a project by going to your projects experiment page

<video controls loop muted autoPlay>
  <source src="https://sofadocsbucket.s3.us-west-2.amazonaws.com/assets/Tutorials/train-script/Create+Experiment+Site.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>







